@Library('pcloud-jenkins-library') _
//Define global variables
env.DISTRO = ""
def TIMEOUT_SEC
def TIMEOUT_MIN
def E2E_SUMMARY
def ERROR_MESSAGE
def clusterInfoFields = [:]
def clusterInfo = [:]
int FAILS_PER_THRESHOLD
int UNSTALE_PER_THRESHOLD
boolean INFRA_ISSUE = false
env.BASTION_IP = ""
boolean DEPLOYMENT_STATUS = false
def REAL_TIME_NS
def USER_TIME_NS
def SYSTEM_TIME_NS
def REAL_TIME_DP
def USER_TIME_DP
def SYSTEM_TIME_DP

pipeline {
    agent {
        dockerfile {
            dir 'images/powervs'
            additionalBuildArgs '--force-rm	--no-cache'
            args '-v /etc/resolv.conf:/etc/resolv.conf'
            label 'daily-x86_64'
        }
    }
    environment {
        //users and credentials. All must be defined in Jenkins Credentials
        GITHUB_USER = credentials('GITHUB_USER')
        TF_VAR_user_name = credentials('GITHUB_USER')
        DOCKER_USER = credentials('DOCKER_USER')
        ARTIFACTORY_USER = credentials('ARTIFACTORY_USER')
        TF_VAR_repo_user = credentials('GITHUB_USER')
        TF_VAR_password = credentials('TF_VAR_password')
        IBMCLOUD_API_KEY = credentials('IBMCLOUD_API_KEY')
        GITHUB_TOKEN = credentials('GITHUB_TOKEN')
        ARTIFACTORY_TOKEN = credentials('ARTIFACTORY_TOKEN')
        TF_VAR_offline_remote_password = credentials('ARTIFACTORY_TOKEN')
        REDHAT_USERNAME = credentials('REDHAT_USERNAME')
        REDHAT_PASSWORD = credentials('REDHAT_PASSWORD')
        PUBLIC_GITHUB_USER = credentials('PUBLIC_GITHUB_USER')
        PUBLIC_GITHUB_TOKEN = credentials('PUBLIC_GITHUB_TOKEN')


        //Env constants
        TERRAFORM_VER = "0.13.4"
        IBM_CLOUD_REGION = "tok"
        IBM_CLOUD_ZONE = "tok04"
        SERVICE_INSTANCE_ID = "e4bb3d9d-a37c-4b1f-a923-4537c0c8beb3"

        BASTION_MEMORY = "8"
        BASTION_PROCESSORS = ".5"

        BOOTSTRAP_MEMORY = "16"
        BOOTSTRAP_PROCESSORS = ".5"

        MASTER_MEMORY = "16"
        MASTER_PROCESSORS = ".5"
        NUM_OF_MASTERS = "3"

        WORKER_MEMORY = "16"
        WORKER_PROCESSORS = ".5"
        NUM_OF_WORKERS = "3"

        BASTION_IMAGE_NAME = "rhel-83-11242020"
        RHCOS_IMAGE_NAME = "rhcos-47-10172020"

        SYSTEM_TYPE = "s922"
        NETWORK_NAME = "ocp-net"
        RHEL_USERNAME = "root"
        RHEL_SMT = "4"
        CLUSTER_DOMAIN = "redhat.com"
        INSTANCE_NAME = "ltccci"

        ENABLE_LOCAL_REGISTRY = "false"
        LOCAL_REGISTRY_IMAGE = "docker.io/ibmcom/registry-ppc64le:2.6.2.5"

        SETUP_SQUID_PROXY = "true"
        SCALE_NUM_OF_DEPLOYMENTS = "60"
        SCALE_NUM_OF_NAMESPACES = "1000"

        // Bellow 4 variables are not used. Disabled in template
        HELPERNODE_REPO = "https://github.com/RedHatOfficial/ocp4-helpernode"
        HELPERNODE_TAG = "5eab3db53976bb16be582f2edc2de02f7510050d"
        INSTALL_PLAYBOOK_REPO = "https://github.com/ocp-power-automation/ocp4-playbooks"
        INSTALL_PLAYBOOT_TAG = "d2509c4b4a67879daa6338f68e8e7eb1e15d05e2"

        UPGRADE_IMAGE = ""
        UPGRADE_PAUSE_TIME = ""
        UPGRADE_DELAY_TIME = ""

        TIMEOUT = "0"
        OCP_RELEASE = "4.7"

        //Makefile variables
        TERRAFORM_FORCE_KEYPAIR_CREATION = "0" //For not using build-barnes
        OPENSHIFT_POWERVS_GIT_TF_DEPLOY_BRANCH="master" //The download branch

        TARGET = "deploy-openshift4-powervs"
        TERMPLATE_FILE = ".${TARGET}.tfvars.template"

        //To pick build-harness . Remove once the Makefile.openshift_pvc upstreamed
        BUILD_HARNESS_ORG="powercloud-cicd"
        //BUILD_HARNESS_BRANCH="devel"
        POWERVS = true
        WAIT_FOR_DEBUG = "0"
	 }

    stages {
        stage('Clone Scale Repo') {
            steps {
                 checkout([$class: 'GitSCM', branches: [[name: '*/master']], doGenerateSubmoduleConfigurations: false, extensions: [[$class: 'RelativeTargetDirectory', relativeTargetDir: 'OpenShift4-tools'], [$class: 'CleanBeforeCheckout'], [$class: 'CloneOption', depth: 0, noTags: false, reference: '', shallow: false, timeout: 20]], submoduleCfg: [], userRemoteConfigs: [[url: 'https://github.com/RobertKrawitz/OpenShift4-tools.git']]])
            }
        }
        stage('pull artifact') {
            steps {
                script {
                    step([  $class: 'CopyArtifact',
                    filter: 'latest-4.7-build.txt',
                    fingerprintArtifacts: true,
                    projectName: 'mirror-openshift-release',
                    target: 'artifactory',
                    selector: lastSuccessful()
                    ])
                }
            }
        }
        //Checkout the installer git repo
        stage('Prepare Teraform Template') {
            steps {
                script {
                    ansiColor('xterm') {
                        echo ""
                    }
                    try
                    {
                        env.OPENSHIFT_IMAGE = ""
                        env.OCP_RELEASE_TAG = ""
                        if (fileExists('artifactory/latest-4.7-build.txt')) {
                            env.OPENSHIFT_IMAGE = readFile 'artifactory/latest-4.7-build.txt'
                            env.OPENSHIFT_IMAGE = env.OPENSHIFT_IMAGE.trim()
                            env.OCP_RELEASE_TAG = env.OPENSHIFT_IMAGE.split(":")[1].trim()
                        }
                        env.OPENSHIFT_INSTALL_TARBALL=getOpenshiftBuild(OCP_RELEASE)
                        if ("${env.OPENSHIFT_INSTALL_TARBALL}" == "null" )
                        {
                            echo "Unable to find openshift install tarball. falling back to default"
                            env.OPENSHIFT_INSTALL_TARBALL="https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview/4.6.0-0.nightly-ppc64le-2020-10-03-224643/openshift-install-linux.tar.gz"
                        }
                        env.OPENSHIFT_CLIENT_TARBALL=getOpenshiftClient(OCP_RELEASE)
                        if ("${env.OPENSHIFT_CLIENT_TARBALL}" == "null")
                        {
                            env.OPENSHIFT_CLIENT_TARBALL="https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview/4.6.0-0.nightly-ppc64le-2020-10-03-224643/openshift-client-linux.tar.gz"
                        }
                        env.OPENSHIFT_CLIENT_TARBALL_AMD64=getOpenshiftClientAMD(OCP_RELEASE)
                        if ("${env.OPENSHIFT_CLIENT_TARBALL_AMD64}" == "null")
                        {
                            env.OPENSHIFT_CLIENT_TARBALL_AMD64="https://mirror.openshift.com/pub/openshift-v4/clients/ocp-dev-preview/4.6.0-0.nightly-2020-10-03-051134/openshift-client-linux.tar.gz"
                        }
                    }
                    catch (err)
                    {
                        echo 'Error ! Template prepration failed !'
                        getArtifactsAndCleanOcp4(env.AUTH_URL)
                        throw err
                    }
                }
            }
        }
        stage('Setup terraform plugin') {
            steps {
                script {
                    ansiColor('xterm') {
                        echo ""
                    }
                    try {
                        sh '''
                        echo ' get the plugin from artifactory repo !'
                        wget https://github.com/IBM-Cloud/terraform-provider-ibm/releases/download/v1.9.0/linux_amd64.zip
                        mkdir -p ~/.terraform.d/plugins/linux_amd64/
                        unzip linux_amd64.zip -d ~/.terraform.d/plugins/linux_amd64/
                        '''
                        }
                    catch (err) {
                        echo 'Error ! ENV setup failed!'
                        getArtifactsAndCleanOcp4(env.AUTH_URL)
                        throw err
                    }
                }
            }
        }
        stage('Initilize Environment') {
            steps {
                script {
                    ansiColor('xterm') {
                        echo ""
                    }
                    try {
                        sh '''
                        echo 'Initializing supporting repos and keys !'

                        cd ${WORKSPACE}/deploy
                        make init
                        make keys
                        make setup-dependencies
                        '''
                        }
                    catch (err) {
                        echo 'Error ! ENV setup failed!'
                        getArtifactsAndCleanOcp4(env.AUTH_URL)
                        throw err
                    }
                }
            }
        }
        stage('Deploy OCP Cluster') {
            steps {
                script {
                    ansiColor('xterm') {
                        echo ""
                    }
                    try {
                        sh '''
                        echo 'Deploying Cluster!'
                        export TF_VAR_offline_remote_password=$TF_VAR_password
                        cd ${WORKSPACE}/deploy
                        make $TARGET || true
                        retries=0
                        until [ "$retries" -ge 3 ]
                        do
                                if [ "$retries" -eq 2 ]; then
                                        make $TARGET:redeploy
                                        sleep 60
                                else
                                        make $TARGET:redeploy || true
                                fi
                                retries=$((retries+1))
                                sleep 10
                        done
                        '''
                        env.BASTION_IP=sh(returnStdout: true, script: "cd ${WORKSPACE}/deploy && make terraform:output TERRAFORM_DIR=.${TARGET} TERRAFORM_OUTPUT_VAR=bastion_public_ip").trim()
                        DEPLOYMENT_STATUS = true
                        }
                    catch (err) {
                        TIMEOUT_HRS =  WAIT_FOR_DEBUG.toInteger()
                        if ( TIMEOUT_HRS != 0 )
                        {
                            TIMEOUT_SEC=TIMEOUT_HRS*60*60
                        }
                        echo "HOLDING THE CLUSTER FOR DEBUGGING, FOR ${TIMEOUT_HRS} MINUTES"
                        sleep TIMEOUT_SEC
                        throw err
                    }
                }
            }
        }

        stage('Run crontab script for capturing outputs of multiple commands') {
            steps {
                script {
                    ansiColor('xterm') {
                        echo ""
                    }
                    try {
                        sh '''
                            cd ${WORKSPACE}/deploy
                            scp -o 'StrictHostKeyChecking no' -i id_rsa ${WORKSPACE}/hack/cron.sh root@${BASTION_IP}:
                            ssh -o 'StrictHostKeyChecking no' -i id_rsa root@${BASTION_IP} "chmod 755 cron.sh;
                                                                                            echo '0 */2 * * * root ~/cron.sh >> ~/cron.log 2>&1' >> /etc/crontab;
                                                                                            exit"
                        '''
                    } catch (err) {
                        echo 'Running Crontab script failed!'
                        getArtifactsAndCleanOcp4(env.AUTH_URL)
                        throw err
                    }
                }
            }
        }

        stage('Run scale test') {
            steps {
                script {
                    ansiColor('xterm') {
                        echo ""
                    }
                    try {
                        writeFile file: 'ingress.yaml', text: 'spec:\n  nodePlacement:\n    nodeSelector:\n      matchLabels:\n        node-role.kubernetes.io/infra: ""'
                        sh 'cat ingress.yaml'
                        writeFile file: 'registry.yaml', text: 'spec:\n  nodeSelector:\n    node-role.kubernetes.io/infra: ""'
                        sh 'cat registry.yaml'
                        sh '''
                        cd ${WORKSPACE}/deploy
                        scp -o 'StrictHostKeyChecking no' -i id_rsa ${WORKSPACE}/ingress.yaml root@${BASTION_IP}:
                        scp -o 'StrictHostKeyChecking no' -i id_rsa ${WORKSPACE}/registry.yaml root@${BASTION_IP}:
                        ssh -o 'StrictHostKeyChecking no' -i id_rsa root@${BASTION_IP} "yum update -y;
                        yum install -y npm time;
                        npm install -g @alexlafroscia/yaml-merge;
                        git clone https://${PUBLIC_GITHUB_TOKEN}@github.com/RobertKrawitz/OpenShift4-tools.git;
                        cd ~/OpenShift4-tools;
                        echo './clusterbuster -N ${SCALE_NUM_OF_NAMESPACES} -d 0 ;while [ `oc get ns | grep clusterbuster | grep -v Active | wc -l` -ne 0 ];do sleep .0000000001;done' > ./namespaces.sh;
                        chmod +x ./namespaces.sh;
                        /usr/bin/time  -o ~/time_taken_namespaces -f  'real-%E user-%U system-%S' ./namespaces.sh;
                        ./clusterbuster --cleanup;

                        sleep 60;

                        oc label node worker-0 node-role.kubernetes.io/infra="";
                        oc label node worker-1 node-role.kubernetes.io/infra="";

                        oc get nodes;

                        oc get ingresscontroller default -n openshift-ingress-operator -o yaml > base_1.yaml && yaml-merge base_1.yaml ~/ingress.yaml | kubectl apply -f - ;
                        oc get pod -n openshift-ingress -o wide;
                        sleep 600;
                        oc get pod -n openshift-ingress -o wide;
                        oc get configs.imageregistry.operator.openshift.io cluster -o yaml > base_2.yaml && yaml-merge base_2.yaml ~/registry.yaml | kubectl apply -f - ;
                        oc get pods -o wide -n openshift-image-registry ;
                        sleep 900 ;
                        oc get pods -o wide -n openshift-image-registry ;
                        echo './clusterbuster -d ${SCALE_NUM_OF_DEPLOYMENTS} -B ltccci ;while [ `oc get pods -n ltccci-0 | grep -v NAME | grep -v Running | wc -l` -ne 0 ];do sleep .0000000001;done' > deployments.sh;
                        chmod +x deployments.sh ;
                        /usr/bin/time -o ~/time_taken_deployments -f  'real-%E user-%U system-%S'  sh deployments.sh ;
                        ./clusterbuster --cleanup -B ltccci ;

                        oc get nodes ;
                        echo 'Get the Cluster Operators' ;
                        oc get co "
                        '''
                        }
                    catch (err) {
                        echo 'Error ! Tearing off the cluster !'
                        getArtifactsAndCleanOcp4(env.AUTH_URL)
                        throw err
                    }
                }
            }
        }

        stage('Gather post pprof and prometheus data') {
            steps {
                script {
                    ansiColor('xterm') {
                        echo ""
                    }
                    try {
                        sh '''
                           ssh -o 'StrictHostKeyChecking no' -i ${WORKSPACE}/deploy/id_rsa root@${BASTION_IP} "oc get --raw /debug/pprof/profile --as=system:admin" > cpu-post.pprof || true
                           ssh -o 'StrictHostKeyChecking no' -i ${WORKSPACE}/deploy/id_rsa root@${BASTION_IP} "oc get --raw /debug/pprof/heap --as=system:admin" > heap-post.pprof || true
                           ssh -o 'StrictHostKeyChecking no' -i ${WORKSPACE}/deploy/id_rsa root@${BASTION_IP} "oc --insecure-skip-tls-verify exec -n openshift-monitoring prometheus-k8s-0 -- tar cvzf - -C /prometheus . " > prometheus.post.tar.gz || true
                        '''
                        }
                    catch (err) {
                        echo 'Error ! Tearing off the cluster !'
                        getArtifactsAndCleanOcp4(env.AUTH_URL)
                        throw err
                    }
                }
            }
        }
    }
    post {
        always {
            getArtifactsAndCleanOcp4(env.AUTH_URL)
            archiveArtifacts allowEmptyArchive: true, artifacts: 'deploy/time_taken_namespaces', fingerprint: true, onlyIfSuccessful: false
            archiveArtifacts allowEmptyArchive: true, artifacts: 'deploy/time_taken_deployments', fingerprint: true, onlyIfSuccessful: false
            archiveArtifacts allowEmptyArchive: true, artifacts: 'deploy/powervc.tfvars', fingerprint: true, onlyIfSuccessful: false
            archiveArtifacts allowEmptyArchive: true, artifacts: 'cpu-post.pprof', fingerprint: true, onlyIfSuccessful: false
            archiveArtifacts allowEmptyArchive: true, artifacts: 'heap-post.pprof', fingerprint: true, onlyIfSuccessful: false
            archiveArtifacts allowEmptyArchive: true, artifacts: 'prometheus.post.tar.gz', fingerprint: true, onlyIfSuccessful: false
            archiveArtifacts allowEmptyArchive: true, artifacts: 'deploy/cron.log', fingerprint: true, onlyIfSuccessful: false
            script {
                if (fileExists('deploy/time_taken_namespaces') && fileExists('deploy/time_taken_namespaces')) {
                    SCALE_SUMMARY = readFile 'deploy/time_taken_namespaces'
                    SCALE_SUMMARY.split('\n').each { line ->
                        if ( line  != null) {
                            if (line.contains('real')){
                                REAL_TIME_NS = line.split()[0].split("-")[1]
                            }
                            if (line.contains('user')){
                                USER_TIME_NS = line.split()[1].split("-")[1]
                            }
                            if (line.contains('sys')){
                                SYSTEM_TIME_NS = line.split()[2].split("-")[1]
                            }
                        }
                    }
                    SCALE_SUMMARY = readFile 'deploy/time_taken_deployments'
                    SCALE_SUMMARY.split('\n').each { line ->
                        if ( line  != null) {
                            if (line.contains('real')){
                                REAL_TIME_DP = line.split()[0].split("-")[1]
                            }
                            if (line.contains('user')){
                                USER_TIME_DP = line.split()[1].split("-")[1]
                            }
                            if (line.contains('sys')){
                                SYSTEM_TIME_DP = line.split()[2].split("-")[1]
                            }
                        }
                    }
                SUMMARY = "Namespace time: " + REAL_TIME_NS + ", " + "Deployment time: " + REAL_TIME_DP
                }
                else
                {
                    step([$class: 'JUnitResultArchiver', allowEmptyResults: true,  testResults: 'hack/dummay-test-summary.xml'])
                    currentBuild.result = 'FAILURE'
                    SUMMARY = "Scale test didn't run"
                }
                def logContent = Jenkins.getInstance().getItemByFullName(env.JOB_NAME).getBuildByNumber(Integer.parseInt(env.BUILD_NUMBER)).logFile.text
                def logContent_modified=logContent.toLowerCase()
                def infra_errors = readFile 'files/infra-issues.txt'
                infra_errors.split('\n').each { line ->
                   line1=line.toLowerCase()
                    if ( line1  != null) {
                        if (logContent_modified.contains(line1)){
                            if ( ! DEPLOYMENT_STATUS ){
                                INFRA_ISSUE = true
                                ERROR_MESSAGE = line1
                            }
                        }
                    }
                }
                if ( env.OPENSHIFT_IMAGE != ""  ) {
                    env.OPENSHIFT_INSTALL_TARBALL = env.OPENSHIFT_IMAGE
                }
                clusterInfo['ocp_build'] = env.OPENSHIFT_INSTALL_TARBALL
                clusterInfo['master_node_cpu'] = "${MASTER_PROCESSORS}"
                clusterInfo['master_node_mem'] = "${MASTER_MEMORY}"
                clusterInfo['worker_node_cpu'] = "${WORKER_PROCESSORS}"
                clusterInfo['worker_node_mem'] = "${WORKER_MEMORY}"
                clusterInfo['cluster_masters'] = "${NUM_OF_MASTERS}"
                clusterInfo['cluster_workers'] = "${NUM_OF_WORKERS}"
                clusterInfo['system_type']     = "${SYSTEM_TYPE}"
                clusterInfo['coreos_build'] = env.RHCOS_IMAGE_NAME
                clusterInfo['real_time_namespace'] = REAL_TIME_NS
                clusterInfo['user_time_namespace'] = USER_TIME_NS
                clusterInfo['system_time_namespace'] = SYSTEM_TIME_NS
                clusterInfo['namespaces'] = "${SCALE_NUM_OF_NAMESPACES}"
                clusterInfo['real_time_deployments'] = REAL_TIME_DP
                clusterInfo['user_time_deployments'] = USER_TIME_DP
                clusterInfo['system_time_deployments'] = SYSTEM_TIME_DP
                clusterInfo['deployments'] = "${SCALE_NUM_OF_DEPLOYMENTS}"
                clusterInfoFields['clusterinfo'] = clusterInfo

                if ( ! INFRA_ISSUE ) {
                    step([$class: 'InfluxDbPublisher', selectedTarget: 'influxdbmollypowervsscale', customDataMap: clusterInfoFields])
                }
                else{
                    echo "Skipping this run from updating the dashboard database, as this is an infra related issue"
                    SUMMARY = ERROR_MESSAGE
                }
            notifyBySlackOcp4(currentBuild.result, env.OPENSHIFT_INSTALL_TARBALL, SUMMARY, env.RHCOS_IMAGE_NAME)
            cleanWs()
            }
        }
    }
}
